{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ur8xi4C7S06n"
   },
   "outputs": [],
   "source": [
    "# Copyright 2024 Google LLC\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     https://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JAPoU8Sm5E6e"
   },
   "source": [
    "# Starry Net Forecasting Pipeline\n",
    "\n",
    "<table align=\"left\">\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://colab.research.google.com/github/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/forecasting/starry_net_pipeline.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/colab-logo-32px.png\" alt=\"Google Colaboratory logo\"><br> Open in Colab\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/colab/import/https:%2F%2Fraw.githubusercontent.com%2FGoogleCloudPlatform%2Fvertex-ai-samples%2Fmain%2Fnotebooks%2Fofficial%2Fforecasting%2Fstarry_net_pipeline.ipynb\">\n",
    "      <img width=\"32px\" src=\"https://cloud.google.com/ml-engine/images/colab-enterprise-logo-32px.png\" alt=\"Google Cloud Colab Enterprise logo\"><br> Open in Colab Enterprise\n",
    "    </a>\n",
    "  </td>    \n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://console.cloud.google.com/vertex-ai/workbench/deploy-notebook?download_url=https://raw.githubusercontent.com/GoogleCloudPlatform/vertex-ai-samples/main/notebooks/official/forecasting/starry_net_pipeline.ipynb\">\n",
    "      <img src=\"https://lh3.googleusercontent.com/UiNooY4LUgW_oTvpsNhPpQzsstV5W8F7rYgxgGBD85cWJoLmrOzhVs_ksK_vgx40SHs7jCqkTkCk=e14-rj-sc0xffffff-h130-w32\" alt=\"Vertex AI logo\"><br> Open in Workbench\n",
    "    </a>\n",
    "  </td>\n",
    "  <td style=\"text-align: center\">\n",
    "    <a href=\"https://github.com/GoogleCloudPlatform/vertex-ai-samples/blob/main/notebooks/official/forecasting/starry_net_pipeline.ipynb\">\n",
    "      <img src=\"https://cloud.google.com/ml-engine/images/github-logo-32px.png\" alt=\"GitHub logo\"><br> View on GitHub\n",
    "    </a>\n",
    "  </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d95487acedbc"
   },
   "source": [
    "### Overview\n",
    "\n",
    "In this tutorial, you learn how to create a Starry Net forecasting model using [Vertex AI Pipelines](https://cloud.google.com/vertex-ai/docs/pipelines/introduction) downloaded from [Google Cloud Pipeline Components](https://cloud.google.com/vertex-ai/docs/pipelines/components-introduction) (GCPC). Starry Net is a state-of-the-art forecaster developed and used internally by Google. Starry Net is a glass-box neural network inspired by statistical time series models, capable of cleaning step changes and spikes, modeling seasonality and events, forecasting trend, and providing both point and prediction interval forecasts in a single, lightweight model. Starry Net stands out among neural network based forecasting models by providing the explainability, interpretability and tunability of traditional statistical forecasters. For example, it features time series feature decomposition and damped local linear exponential smoothing model as the trend structure.\n",
    "\n",
    "This tutorial uses the following Google Cloud ML services:\n",
    "\n",
    "- AutoML training\n",
    "- Vertex AI Pipelines\n",
    "\n",
    "The steps performed are:\n",
    "\n",
    "- Create a training pipeline with Starry Net.\n",
    "- Perform the batch prediction using the trained model in the above step.\n",
    "- Deploy the trained model to an endpoint to perform online predictions and generate decomposition plots."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2475eb388888"
   },
   "source": [
    "### Dataset\n",
    "\n",
    "This tutorial uses the [Liquor dataset](https://www.kaggle.com/datasets/residentmario/iowa-liquor-sales), which forecasts the alcoholic beverage sales in the Midwest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "121b6cc45fc0"
   },
   "source": [
    "### Costs\n",
    "\n",
    "This tutorial uses billable components of Google Cloud:\n",
    "\n",
    "* Vertex AI\n",
    "* Cloud Storage\n",
    "* BigQuery\n",
    "* Dataflow\n",
    "\n",
    "Learn about [Vertex AI\n",
    "pricing](https://cloud.google.com/vertex-ai/pricing), [Cloud Storage\n",
    "pricing](https://cloud.google.com/storage/pricing), and [BigQuery](https://cloud.google.com/bigquery), and use the [Pricing\n",
    "Calculator](https://cloud.google.com/products/calculator/)\n",
    "to generate a cost estimate based on your projected usage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eef41ef4c197"
   },
   "source": [
    "## Install additional packages\n",
    "\n",
    "Install the Google Cloud Pipeline Components (GCPC) SDK not earlier than `2.16.1`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "93b0e8cb6420",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting google-cloud-pipeline-components==2.16.1\n",
      "  Downloading google_cloud_pipeline_components-2.16.1-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting grpcio==1.62.1\n",
      "  Downloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: grpcio-status==1.48.2 in /opt/conda/lib/python3.10/site-packages (1.48.2)\n",
      "Collecting googleapis-common-protos==1.63.0\n",
      "  Downloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Collecting grpc-google-iam-v1==0.13.0\n",
      "  Downloading grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting google-crc32c==1.5.0\n",
      "  Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.3 kB)\n",
      "Collecting kfp==2.7.0\n",
      "  Downloading kfp-2.7.0.tar.gz (441 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting protobuf==4.25.3\n",
      "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
      "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components==2.16.1) (1.34.1)\n",
      "Requirement already satisfied: google-cloud-aiplatform<2,>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components==2.16.1) (1.72.0)\n",
      "Requirement already satisfied: Jinja2<4,>=3.1.2 in /opt/conda/lib/python3.10/site-packages (from google-cloud-pipeline-components==2.16.1) (3.1.4)\n",
      "Requirement already satisfied: click<9,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.7.0) (8.1.7)\n",
      "Requirement already satisfied: docstring-parser<1,>=0.7.3 in /opt/conda/lib/python3.10/site-packages (from kfp==2.7.0) (0.16)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.1 in /opt/conda/lib/python3.10/site-packages (from kfp==2.7.0) (2.36.0)\n",
      "Requirement already satisfied: google-cloud-storage<3,>=2.2.1 in /opt/conda/lib/python3.10/site-packages (from kfp==2.7.0) (2.14.0)\n",
      "Collecting kfp-pipeline-spec==0.3.0 (from kfp==2.7.0)\n",
      "  Downloading kfp_pipeline_spec-0.3.0-py3-none-any.whl.metadata (329 bytes)\n",
      "Requirement already satisfied: kfp-server-api<2.1.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.7.0) (2.0.5)\n",
      "Requirement already satisfied: kubernetes<27,>=8.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.7.0) (26.1.0)\n",
      "Requirement already satisfied: PyYAML<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from kfp==2.7.0) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt<1,>=0.8.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.7.0) (0.10.1)\n",
      "Requirement already satisfied: tabulate<1,>=0.8.6 in /opt/conda/lib/python3.10/site-packages (from kfp==2.7.0) (0.9.0)\n",
      "Requirement already satisfied: urllib3<2.0.0 in /opt/conda/lib/python3.10/site-packages (from kfp==2.7.0) (1.26.20)\n",
      "INFO: pip is looking at multiple versions of google-api-core to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5 (from google-cloud-pipeline-components==2.16.1)\n",
      "  Downloading google_api_core-2.24.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Requirement already satisfied: proto-plus<2.0.0dev,>=1.22.3 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components==2.16.1) (1.25.0)\n",
      "Requirement already satisfied: requests<3.0.0.dev0,>=2.18.0 in /opt/conda/lib/python3.10/site-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components==2.16.1) (2.32.3)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==2.7.0) (5.5.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==2.7.0) (0.4.1)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.1->kfp==2.7.0) (4.9)\n",
      "Requirement already satisfied: packaging>=14.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components==2.16.1) (24.1)\n",
      "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0dev,>=1.15.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components==2.16.1) (3.25.0)\n",
      "Requirement already satisfied: google-cloud-resource-manager<3.0.0dev,>=1.3.3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components==2.16.1) (1.13.1)\n",
      "Requirement already satisfied: shapely<3.0.0dev in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components==2.16.1) (2.0.6)\n",
      "Requirement already satisfied: pydantic<3 in /opt/conda/lib/python3.10/site-packages (from google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components==2.16.1) (2.9.2)\n",
      "Requirement already satisfied: google-cloud-core<3.0dev,>=2.3.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.7.0) (2.4.1)\n",
      "Requirement already satisfied: google-resumable-media>=2.6.0 in /opt/conda/lib/python3.10/site-packages (from google-cloud-storage<3,>=2.2.1->kfp==2.7.0) (2.7.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from Jinja2<4,>=3.1.2->google-cloud-pipeline-components==2.16.1) (3.0.2)\n",
      "Requirement already satisfied: six>=1.10 in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.7.0) (1.16.0)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.7.0) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil in /opt/conda/lib/python3.10/site-packages (from kfp-server-api<2.1.0,>=2.0.0->kfp==2.7.0) (2.9.0.post0)\n",
      "Requirement already satisfied: setuptools>=21.0.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp==2.7.0) (75.3.0)\n",
      "Requirement already satisfied: websocket-client!=0.40.0,!=0.41.*,!=0.42.*,>=0.32.0 in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp==2.7.0) (1.8.0)\n",
      "Requirement already satisfied: requests-oauthlib in /opt/conda/lib/python3.10/site-packages (from kubernetes<27,>=8.0.0->kfp==2.7.0) (2.0.0)\n",
      "INFO: pip is looking at multiple versions of google-api-core[grpc] to determine which version is compatible with other requirements. This could take a while.\n",
      "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /opt/conda/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.1->kfp==2.7.0) (0.6.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components==2.16.1) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.23.4 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components==2.16.1) (2.23.4)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /opt/conda/lib/python3.10/site-packages (from pydantic<3->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components==2.16.1) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components==2.16.1) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3.0.0.dev0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.0,<3.0.0dev,>=1.31.5->google-cloud-pipeline-components==2.16.1) (3.10)\n",
      "Requirement already satisfied: numpy<3,>=1.14 in /opt/conda/lib/python3.10/site-packages (from shapely<3.0.0dev->google-cloud-aiplatform<2,>=1.14.0->google-cloud-pipeline-components==2.16.1) (1.26.4)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib->kubernetes<27,>=8.0.0->kfp==2.7.0) (3.2.2)\n",
      "Downloading google_cloud_pipeline_components-2.16.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading grpcio-1.62.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m116.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading googleapis_common_protos-1.63.0-py2.py3-none-any.whl (229 kB)\n",
      "Downloading grpc_google_iam_v1-0.13.0-py2.py3-none-any.whl (25 kB)\n",
      "Downloading google_crc32c-1.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32 kB)\n",
      "Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
      "Downloading kfp_pipeline_spec-0.3.0-py3-none-any.whl (12 kB)\n",
      "Downloading google_api_core-2.24.0-py3-none-any.whl (158 kB)\n",
      "Building wheels for collected packages: kfp\n",
      "  Building wheel for kfp (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kfp: filename=kfp-2.7.0-py3-none-any.whl size=610410 sha256=3d4291866b51db80f3d63f78a177e72319ad10cae0f44088d87734b93cf0363b\n",
      "  Stored in directory: /home/jupyter/.cache/pip/wheels/9e/7d/a4/f9d013e82681c9746ef10de3b00456163577a99279c5ed673d\n",
      "Successfully built kfp\n",
      "Installing collected packages: protobuf, grpcio, google-crc32c, kfp-pipeline-spec, googleapis-common-protos, google-api-core, grpc-google-iam-v1, kfp, google-cloud-pipeline-components\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 3.20.3\n",
      "    Uninstalling protobuf-3.20.3:\n",
      "      Successfully uninstalled protobuf-3.20.3\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/lib/python3.10/site-packages/google/~rotobuf'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: grpcio\n",
      "    Found existing installation: grpcio 1.68.0\n",
      "    Uninstalling grpcio-1.68.0:\n",
      "      Successfully uninstalled grpcio-1.68.0\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/lib/python3.10/site-packages/~rpc'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: google-crc32c\n",
      "    Found existing installation: google-crc32c 1.6.0\n",
      "    Uninstalling google-crc32c-1.6.0:\n",
      "      Successfully uninstalled google-crc32c-1.6.0\n",
      "  Attempting uninstall: kfp-pipeline-spec\n",
      "    Found existing installation: kfp-pipeline-spec 0.2.2\n",
      "    Uninstalling kfp-pipeline-spec-0.2.2:\n",
      "      Successfully uninstalled kfp-pipeline-spec-0.2.2\n",
      "  Attempting uninstall: googleapis-common-protos\n",
      "    Found existing installation: googleapis-common-protos 1.66.0\n",
      "    Uninstalling googleapis-common-protos-1.66.0:\n",
      "      Successfully uninstalled googleapis-common-protos-1.66.0\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/lib/python3.10/site-packages/google/~pi'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/lib/python3.10/site-packages/google/~ongrunning'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/lib/python3.10/site-packages/google/~pc'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/lib/python3.10/site-packages/google/~ype'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: google-api-core\n",
      "    Found existing installation: google-api-core 1.34.1\n",
      "    Uninstalling google-api-core-1.34.1:\n",
      "      Successfully uninstalled google-api-core-1.34.1\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/opt/conda/lib/python3.10/site-packages/google/~pi_core'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: grpc-google-iam-v1\n",
      "    Found existing installation: grpc-google-iam-v1 0.13.1\n",
      "    Uninstalling grpc-google-iam-v1-0.13.1:\n",
      "      Successfully uninstalled grpc-google-iam-v1-0.13.1\n",
      "  Attempting uninstall: kfp\n",
      "    Found existing installation: kfp 2.5.0\n",
      "    Uninstalling kfp-2.5.0:\n",
      "      Successfully uninstalled kfp-2.5.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "google-api-python-client 1.8.0 requires google-api-core<2dev,>=1.13.0, but you have google-api-core 2.24.0 which is incompatible.\n",
      "google-cloud-datastore 1.15.5 requires protobuf<4.0.0dev, but you have protobuf 4.25.3 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-api-core-2.24.0 google-cloud-pipeline-components-2.16.1 google-crc32c-1.5.0 googleapis-common-protos-1.63.0 grpc-google-iam-v1-0.13.0 grpcio-1.62.1 kfp-2.7.0 kfp-pipeline-spec-0.3.0 protobuf-4.25.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install --upgrade \\\n",
    "    google-cloud-pipeline-components==2.16.1 \\\n",
    "    grpcio==1.62.1 \\\n",
    "    grpcio-status==1.48.2 \\\n",
    "    googleapis-common-protos==1.63.0 \\\n",
    "    grpc-google-iam-v1==0.13.0 \\\n",
    "    google-crc32c==1.5.0 \\\n",
    "    kfp==2.7.0 \\\n",
    "    protobuf==4.25.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5b08ba354c6e"
   },
   "source": [
    "## Restart runtime (Colab only)\n",
    "\n",
    "To use the newly installed packages, you must restart the runtime on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bea801acf6b5"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "if \"google.colab\" in sys.modules:\n",
    "\n",
    "    import IPython\n",
    "\n",
    "    app = IPython.Application.instance()\n",
    "    app.kernel.do_shutdown(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "61RBz8LLbxCR"
   },
   "source": [
    "## Before you begin\n",
    "\n",
    "### Set up your Google Cloud project\n",
    "\n",
    "**The following steps are required, regardless of your notebook environment.**\n",
    "\n",
    "1. [Select or create a Google Cloud project](https://console.cloud.google.com/cloud-resource-manager).\n",
    "\n",
    "2. [Make sure that billing is enabled for your project](https://cloud.google.com/billing/docs/how-to/modify-project).\n",
    "\n",
    "3. [Enable the Vertex AI API](https://console.cloud.google.com/flows/enableapi?apiid=ml.googleapis.com,dataflow.googleapis.com,compute_component,storage-component.googleapis.com).\n",
    "\n",
    "4. If you are running this notebook locally, you need to install the [Cloud SDK](https://cloud.google.com/sdk)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46525f0f9af2"
   },
   "source": [
    "## Notes about service account and permission\n",
    "\n",
    "For full details of the permission setup, refer to https://cloud.google.com/vertex-ai/docs/tabular-data/tabular-workflows/service-accounts\n",
    "\n",
    "**By default no configuration is required**, if you run into any permission related issue, please make sure the service accounts above have the required roles:\n",
    "\n",
    "|Service account email|Description|Roles|\n",
    "|---|---|---|\n",
    "|PROJECT_NUMBER-compute@developer.gserviceaccount.com|Compute Engine default service account|Dataflow Developer, Dataflow Worker, Storage Admin, BigQuery Data Editor, Vertex AI User, Service Account User|\n",
    "|service-PROJECT_NUMBER@gcp-sa-aiplatform.iam.gserviceaccount.com|AI Platform Service Agent|Vertex AI Service Agent|\n",
    "\n",
    "\n",
    "1. Goto https://console.cloud.google.com/iam-admin/iam.\n",
    "2. Check the \"Include Google-provided role grants\" checkbox.\n",
    "3. Find the above emails.\n",
    "4. Grant the corresponding roles.\n",
    "\n",
    "### Using data source from a different project\n",
    "- For the BQ data source, grant both service accounts the \"BigQuery Data Viewer\" role.\n",
    "- For the CSV data source, grant both service accounts the \"Storage Object Viewer\" role.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a09d2923c71d",
    "tags": []
   },
   "source": [
    "### Set your project ID\n",
    "\n",
    "**If you don't know your project ID**, try the following:\n",
    "* Run `gcloud config list`.\n",
    "* Run `gcloud projects list`.\n",
    "* See the support page: [Locate the project ID](https://support.google.com/googleapi/answer/7014113)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "6d35291c8417",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"247507300617\"  # @param {type:\"string\"}\n",
    "\n",
    "# Set the project id\n",
    "! gcloud config set project {PROJECT_ID}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f68658ebf3c1"
   },
   "source": [
    "### Location\n",
    "\n",
    "You can also change the `LOCATION` variable used by Vertex AI. Learn more about [Vertex AI regions](https://cloud.google.com/vertex-ai/docs/general/locations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "f83bd6013894",
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOCATION = \"us-central1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "08bfd1eb44ef"
   },
   "source": [
    "### Authenticate your Google Cloud account\n",
    "\n",
    "Depending on your Jupyter environment, you may have to manually authenticate. Follow the relevant instructions below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15c8f438ba7d"
   },
   "source": [
    "**1. Vertex AI Workbench**\n",
    "* Do nothing since you're already authenticated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ad1138a125ea"
   },
   "source": [
    "**2. Local JupyterLab instance, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ce6043da7b33",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "You are running on a Google Compute Engine virtual machine.\n",
      "It is recommended that you use service accounts for authentication.\n",
      "\n",
      "You can run:\n",
      "\n",
      "  $ gcloud config set account `ACCOUNT`\n",
      "\n",
      "to switch accounts if necessary.\n",
      "\n",
      "Your credentials may be visible to others with access to this\n",
      "virtual machine. Are you sure you want to authenticate with\n",
      "your personal account?\n",
      "\n",
      "Do you want to continue (Y/n)?  "
     ]
    }
   ],
   "source": [
    "# ! gcloud auth login"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0367eac06a10"
   },
   "source": [
    "**3. Colab, uncomment and run:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "21ad4dbb4a61"
   },
   "outputs": [],
   "source": [
    "# from google.colab import auth\n",
    "# auth.authenticate_user()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "703d16eb78ce"
   },
   "source": [
    "## Import libraries and define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e6fb4ce31bb5",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import required modules\n",
    "\n",
    "import math\n",
    "import os\n",
    "import time\n",
    "from typing import Any, Dict, List, Optional\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from google.cloud import aiplatform, bigquery\n",
    "from google_cloud_pipeline_components.preview.starry_net import starry_net\n",
    "from kfp import compiler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bad00c4f2540"
   },
   "source": [
    "## Initialize Vertex AI SDK for Python\n",
    "\n",
    "Initialize the Vertex SDK for Python for your project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ee974fa8d299",
    "tags": []
   },
   "outputs": [],
   "source": [
    "aiplatform.init(project=PROJECT_ID, location=LOCATION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "696190db0897"
   },
   "source": [
    "## Create the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "03007ce17984",
    "tags": []
   },
   "outputs": [],
   "source": [
    "now = str(int(time.time()))\n",
    "bigquery_dataset = f\"starry_net_test_{now}\"\n",
    "bigquery_table = \"iowa_weekly_by_store\"\n",
    "\n",
    "client = bigquery.Client()\n",
    "dataset = bigquery.Dataset(f\"{PROJECT_ID}.{bigquery_dataset}\")\n",
    "dataset.location = \"US\"  # Location must be in the multi-region US, since the source dataset is in multi-region US.\n",
    "client.create_dataset(dataset, timeout=30)\n",
    "\n",
    "query = f\"\"\"\n",
    "CREATE OR REPLACE TABLE {PROJECT_ID}.{bigquery_dataset}.{bigquery_table} as\n",
    "SELECT \n",
    "DATE_TRUNC(date, WEEK) as week,\n",
    "SUM(sale_dollars) as sales,\n",
    "CONCAT(store_name, \"|\", store_number) as item,\n",
    "store_name, store_number, city, county,\n",
    "FROM `bigquery-public-data.iowa_liquor_sales.sales`\n",
    "GROUP BY week, item, store_name, store_number, city, county\n",
    "\"\"\"\n",
    "\n",
    "_ = client.query(query).result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e507e8218df0"
   },
   "source": [
    "### Create a Cloud Storage bucket\n",
    "\n",
    "Create a storage bucket to store intermediate artifacts such as datasets, TF model checkpoint, TensorBoard file, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ab5ae3330d54",
    "tags": []
   },
   "outputs": [],
   "source": [
    "BUCKET_URI = f\"gs://{PROJECT_ID}-starry-net-{now}\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aa6e1cfc4236"
   },
   "source": [
    "**If your bucket doesn't already exist**: Run the following cell to create your Cloud Storage bucket.\n",
    "\n",
    "We create the bucket in multi-region US, since the bucket and the Big Query dataset must exist in the same region."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83e8fb03546b",
    "tags": []
   },
   "outputs": [],
   "source": [
    "! gsutil mb -l US -p {PROJECT_ID} {BUCKET_URI}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8694977d4123"
   },
   "source": [
    "## Compile the pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c8a4accb1cb8",
    "tags": []
   },
   "outputs": [],
   "source": [
    "template_path = os.path.join(os.getcwd(), \"starry-net.yaml\")\n",
    "compiler.Compiler().compile(starry_net, template_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aafe89092b0b"
   },
   "source": [
    "## Create TensorBoard Instance for saving decomposition plots\n",
    "\n",
    "NOTE: This can be done in the console. See [documentation](https://cloud.google.com/vertex-ai/docs/experiments/tensorboard-setup#google-cloud-console) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "38ce1f79b84e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_tensorboard(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    display_name: Optional[str] = None,\n",
    "):\n",
    "    aiplatform.init(project=project, location=location)\n",
    "\n",
    "    tensorboard = aiplatform.Tensorboard.create(\n",
    "        display_name=display_name,\n",
    "        project=project,\n",
    "        location=location,\n",
    "    )\n",
    "\n",
    "    aiplatform.init(\n",
    "        project=project, location=location, experiment_tensorboard=tensorboard\n",
    "    )\n",
    "\n",
    "    return tensorboard\n",
    "\n",
    "\n",
    "tensorboard = create_tensorboard(PROJECT_ID, LOCATION, \"starry-net-tensorboard\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3967490ba06c"
   },
   "source": [
    "## Set the pipeline parameters\n",
    "\n",
    "In terms of tuning your model, the most important parameters are\n",
    "\n",
    "```\n",
    "dataprep_backcast_length: The length of the context window to feed into the\n",
    "  model. Generally, this can be somewhere bween 2-4x forecast horizon.\n",
    "  The shorter, the more reactive the model will be.\n",
    "\n",
    "dataprep_forecast_length: The length of the forecast horizon used in the\n",
    "  loss function during training and during evaluation, so that the model is\n",
    "  optimized to produce forecasts from 0 to H.\n",
    "\n",
    "trainer_model_blocks: The list of model blocks to use in the order they will\n",
    "  appear in the model. Possible values are `cleaning`, `change_point`,\n",
    "  `trend`, `hour_of_week`, `day_of_week`, `day_of_year`, `week_of_year`,\n",
    "  `month_of_year`, `residual`.\n",
    "  Model blocks should always include `cleaning`, `change_point`,\n",
    "  `trend`, and `residual`, in that order. If your data has seasonal patterns,\n",
    "  add any of the relevant seasonal blocks between the `trend` and `residual`\n",
    "  blocks.\n",
    "  \n",
    "trainer_cleaning_activation_regularizer_coeff: The L1 regularization\n",
    "  coefficient for the anomaly detection activation in the cleaning block.\n",
    "  The larger the value, the less aggressive the cleaning, so fewer and only\n",
    "  the most extreme anomalies are detected. A rule of thumb is that this\n",
    "  value should be about the same scale of your series.\n",
    "\n",
    "trainer_change_point_activation_regularizer_coeff: The L1 regularization\n",
    "  coefficient for the change point detection activation in the change point\n",
    "  block. The larger the value, the less aggressive the cleaning, so fewer\n",
    "  and only the most extreme change points are detected. A rule of thumb is\n",
    "  that this value should be a ratio of the\n",
    "  trainer_change_point_output_regularizer_coeff to determine the sparsity\n",
    "  of the changes. If you want the model to detect many small step changes\n",
    "  this number should be smaller than the\n",
    "  trainer_change_point_output_regularizer_coeff. To detect fewer large step\n",
    "  changes, this number should be about equal to or larger than the\n",
    "  trainer_change_point_output_regularizer_coeff.\n",
    "\n",
    "trainer_change_point_output_regularizer_coeff: The L2 regularization\n",
    "  penalty applied to the mean lag-one difference of the cleaned output of\n",
    "  the change point block. Intutively,\n",
    "  trainer_change_point_activation_regularizer_coeff determines how many\n",
    "  steps to detect in the series, while this parameter determines how\n",
    "  aggressively to clean the detected steps. The higher this value, the more\n",
    "  aggressive the cleaning. A rule of thumb is that this value should be\n",
    "  about the same scale of your series.\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0f368be012ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "root_dir = os.path.join(BUCKET_URI, f\"automl_forecasting_pipeline/run-{now}\")\n",
    "parameters = {\n",
    "    \"tensorboard_instance_id\": tensorboard.resource_name.split(\"/\")[-1],\n",
    "    \"dataprep_backcast_length\": 80,\n",
    "    \"dataprep_forecast_length\": 12,\n",
    "    \"dataprep_train_end_date\": \"2023-11-01\",\n",
    "    \"dataprep_n_val_windows\": 1,\n",
    "    \"dataprep_n_test_windows\": 1,\n",
    "    \"dataprep_test_set_stride\": 4,\n",
    "    \"dataprep_test_set_bigquery_dataset\": f\"bq://{PROJECT_ID}.{bigquery_dataset}\",\n",
    "    \"dataflow_machine_type\": \"n1-standard-16\",\n",
    "    \"dataflow_max_replica_count\": 50,\n",
    "    \"dataflow_starting_replica_count\": 25,\n",
    "    \"dataflow_disk_size_gb\": 50,\n",
    "    \"dataprep_csv_data_path\": \"\",\n",
    "    \"dataprep_csv_static_covariates_path\": \"\",\n",
    "    \"dataprep_bigquery_data_path\": f\"bq://{PROJECT_ID}.{bigquery_dataset}.{bigquery_table}\",\n",
    "    \"dataprep_ts_identifier_columns\": [\"item\"],\n",
    "    \"dataprep_time_column\": \"week\",\n",
    "    \"dataprep_target_column\": \"sales\",\n",
    "    \"dataprep_static_covariate_columns\": [\"county\", \"city\"],\n",
    "    \"dataprep_previous_run_dir\": \"\",\n",
    "    \"dataprep_nan_threshold\": 0.95,\n",
    "    \"dataprep_zero_threshold\": 0.95,\n",
    "    \"trainer_machine_type\": \"n1-standard-8\",\n",
    "    \"trainer_accelerator_type\": \"NVIDIA_TESLA_V100\",\n",
    "    \"trainer_num_epochs\": 1,\n",
    "    \"trainer_cleaning_activation_regularizer_coeff\": 1e3,\n",
    "    \"trainer_change_point_activation_regularizer_coeff\": 1e3,\n",
    "    \"trainer_change_point_output_regularizer_coeff\": 1e3,\n",
    "    \"trainer_trend_alpha_upper_bound\": 0.5,\n",
    "    \"trainer_trend_beta_upper_bound\": 0.2,\n",
    "    \"trainer_trend_phi_lower_bound\": 0.99,\n",
    "    \"trainer_trend_b_fixed_val\": -1,\n",
    "    \"trainer_trend_b0_fixed_val\": -1,\n",
    "    \"trainer_trend_phi_fixed_val\": -1,\n",
    "    \"trainer_quantiles\": [],\n",
    "    \"trainer_model_blocks\": [\n",
    "        \"cleaning\",\n",
    "        \"change_point\",\n",
    "        \"trend\",\n",
    "        \"week_of_year\",\n",
    "        \"month_of_year\",\n",
    "        \"residual\",\n",
    "    ],\n",
    "    \"tensorboard_n_decomposition_plots\": 5,\n",
    "    \"encryption_spec_key_name\": \"\",\n",
    "    \"location\": LOCATION,\n",
    "    \"project\": PROJECT_ID,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "20ef3cf18408"
   },
   "source": [
    "## Launch the pipeline\n",
    "\n",
    "NOTE: This can be done in the console straight for the [Vertex AI Template Gallery](https://cloud.google.com/vertex-ai/docs/pipelines/use-template-gallery#console) by seraching for the Starry Net pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "373f494b995e",
    "tags": []
   },
   "outputs": [],
   "source": [
    "job_id = f\"starry-net-{now}\"\n",
    "job = aiplatform.PipelineJob(\n",
    "    display_name=job_id,\n",
    "    location=LOCATION,\n",
    "    template_path=template_path,\n",
    "    job_id=job_id,\n",
    "    pipeline_root=root_dir,\n",
    "    parameter_values=parameters,\n",
    "    enable_caching=False,\n",
    ")\n",
    "job.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abb6a1571233"
   },
   "source": [
    "## Create the Model Endpoint and Deploy Model\n",
    "\n",
    "NOTE: This can all be done in the console. See [documentation](https://cloud.google.com/vertex-ai/docs/general/deployment#google-cloud-console) for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1a574066dfd8"
   },
   "outputs": [],
   "source": [
    "endpoint = aiplatform.Endpoint.create(\n",
    "    display_name=\"starry_net_test\",\n",
    "    project=PROJECT_ID,\n",
    "    location=LOCATION,\n",
    ")\n",
    "\n",
    "print(endpoint.display_name)\n",
    "print(endpoint.resource_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aee18ec1e7b2"
   },
   "outputs": [],
   "source": [
    "model_name = None\n",
    "for task in job.task_details:\n",
    "    if \"model-upload\" in task.task_name:\n",
    "        for k, v in task.outputs.items():\n",
    "            if k == \"model\":\n",
    "                model_name = v.artifacts[0].uri.split(\"/v1/\")[1]\n",
    "if model_name is None:\n",
    "    raise ValueError(\n",
    "        \"Failed to find model. Try looking up the name in the Model Registry console.\"\n",
    "    )\n",
    "\n",
    "model = aiplatform.Model(model_name=model_name)\n",
    "\n",
    "model.deploy(\n",
    "    endpoint=endpoint,\n",
    "    deployed_model_display_name=\"My Starry test model\",\n",
    "    traffic_percentage=100,\n",
    "    machine_type=\"n1-standard-8\",\n",
    "    min_replica_count=1,\n",
    "    max_replica_count=1,\n",
    ")\n",
    "\n",
    "model.wait()\n",
    "\n",
    "print(model.display_name)\n",
    "print(model.resource_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "15ed13f653bc"
   },
   "source": [
    "## Helper Functions for Plotting Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ea87bd792967"
   },
   "outputs": [],
   "source": [
    "_COLOR_STYLE = {\n",
    "    \"history\": \"dodgerblue\",\n",
    "    \"input remainder\": \"dodgerblue\",\n",
    "    \"fitted\": \"red\",\n",
    "    \"forecasts\": \"limegreen\",\n",
    "    \"forecasts_P50\": \"limegreen\",\n",
    "    \"forecasts_P95\": \"darkviolet\",\n",
    "}\n",
    "\n",
    "\n",
    "def _plot_forecast(\n",
    "    backcast_len: int,\n",
    "    forecast_len: int,\n",
    "    timestamps: Optional[List[pd.Timestamp]],\n",
    "    history: List[float],\n",
    "    fitted: List[float],\n",
    "    forecasts: List[float],\n",
    "    color_style: Dict[str, str],\n",
    "    ax: Optional[plt.Axes],\n",
    ") -> None:\n",
    "    \"\"\"Plots the overall forecasts from a Starry-Net Vertex Endpoint.\"\"\"\n",
    "    forecasts = [np.nan] * backcast_len + forecasts\n",
    "    fitted = fitted + [np.nan] * forecast_len\n",
    "    history = history + [np.nan] * forecast_len\n",
    "    data = {\"forecasts_P50\": forecasts, \"fitted\": fitted, \"history\": history}\n",
    "    df = pd.DataFrame(data, index=timestamps)\n",
    "    df.plot(\n",
    "        kind=\"line\",\n",
    "        ax=ax,\n",
    "        legend=True,\n",
    "        title=\"Overall forecasts\",\n",
    "        style=color_style,\n",
    "    )\n",
    "\n",
    "\n",
    "def _plot_cleaning_block(\n",
    "    block_name: str,\n",
    "    timestamps: Optional[List[pd.Timestamp]],\n",
    "    input_remainder: List[float],\n",
    "    cleaned_series: List[float],\n",
    "    color_style: Dict[str, str],\n",
    "    legend: bool,\n",
    "    ax: Optional[plt.Axes],\n",
    ") -> None:\n",
    "    \"\"\"Plots a cleaning block's output from a Starry-Net Vertex Endpoint.\"\"\"\n",
    "    data = {\n",
    "        \"input remainder\": input_remainder,\n",
    "        \"fitted\": cleaned_series,\n",
    "    }\n",
    "    df = pd.DataFrame(data, index=timestamps)\n",
    "    df.plot(kind=\"line\", ax=ax, legend=legend, title=block_name, style=color_style)\n",
    "\n",
    "\n",
    "def _plot_intermediate_block(\n",
    "    block_name: str,\n",
    "    backcast_len: int,\n",
    "    forecast_len: int,\n",
    "    timestamps: Optional[List[pd.Timestamp]],\n",
    "    input_remainder: List[float],\n",
    "    fitted: List[float],\n",
    "    forecasts: List[float],\n",
    "    color_style: Dict[str, str],\n",
    "    legend: bool,\n",
    "    ax: Optional[plt.Axes],\n",
    ") -> None:\n",
    "    \"\"\"Plots an intermediate block's output from a Starry-Net Vertex Endpoint.\"\"\"\n",
    "    forecasts = [np.nan] * backcast_len + forecasts\n",
    "    fitted = fitted + [np.nan] * forecast_len\n",
    "    input_remainder = input_remainder + [np.nan] * forecast_len\n",
    "    data = {\n",
    "        \"input remainder\": input_remainder,\n",
    "        \"fitted\": fitted,\n",
    "        \"forecasts\": forecasts,\n",
    "    }\n",
    "    df = pd.DataFrame(data, index=timestamps)\n",
    "    df.plot(kind=\"line\", ax=ax, legend=legend, title=block_name, style=color_style)\n",
    "\n",
    "\n",
    "def plot_forecasts_from_endpoint(\n",
    "    instance: Dict[str, Any],\n",
    "    predictions: Dict[str, Any],\n",
    "    height: float = 2.5,\n",
    "    width: float = 5,\n",
    "    color_style: Optional[Dict[str, str]] = None,\n",
    ") -> None:\n",
    "    \"\"\"Plots forecasts from a Starry-Net Vertex AI endpoint.\n",
    "\n",
    "    Args:\n",
    "      instance: The input instance sent to the end point containing the input time\n",
    "        series and maybe timestamps.\n",
    "      predictions: The dictionary of predictions.\n",
    "      height: height of the subplot in the visualization.\n",
    "      width: width of the subplot in the visualization.\n",
    "      color_style: The map of colors to use for each plot.\n",
    "    \"\"\"\n",
    "    # set up subplot grid\n",
    "    i = 2\n",
    "    j = 0\n",
    "    decomposition = predictions.get(\"decomposition\", {})\n",
    "    blocks = sorted(decomposition.keys())\n",
    "    n_blocks = len(blocks)\n",
    "    row = math.ceil(n_blocks / 2) + 2\n",
    "    fig_size = (2 * width, row * height)\n",
    "    fig, axes = plt.subplots(row, 2, figsize=fig_size, sharey=True)\n",
    "    fig.tight_layout(pad=3.0)\n",
    "    # plot overall forecast\n",
    "    ax_overall = plt.subplot2grid((row, 2), (0, 0), colspan=2, rowspan=2)\n",
    "    axes[0, 0].tick_params(labelbottom=False, labelleft=False)\n",
    "    axes[0, 1].tick_params(labelbottom=False, labelleft=False)\n",
    "    axes[1, 0].tick_params(labelbottom=False, labelleft=False)\n",
    "    axes[1, 1].tick_params(labelbottom=False, labelleft=False)\n",
    "\n",
    "    timestamps = instance.get(\"timestamps\")\n",
    "    backcast_len = len(predictions[\"fitted\"])\n",
    "    forecast_len = len(predictions[\"value\"])\n",
    "    total_len = backcast_len + forecast_len\n",
    "    x = instance[\"x\"]\n",
    "    if len(x) != backcast_len:\n",
    "        raise ValueError(f'instance[\"x\"] should be of len {backcast_len}')\n",
    "    if timestamps is not None:\n",
    "        timestamps = [pd.to_datetime(ts) for ts in timestamps]\n",
    "        if len(timestamps) != total_len:\n",
    "            raise ValueError(f'instance[\"timestamps\"] should be of len {total_len}')\n",
    "    else:\n",
    "        timestamps = (\n",
    "            range(backcast_len + forecast_len) if timestamps is None else timestamps\n",
    "        )\n",
    "\n",
    "    _plot_forecast(\n",
    "        backcast_len,\n",
    "        forecast_len,\n",
    "        timestamps,\n",
    "        x,\n",
    "        predictions[\"fitted\"],\n",
    "        predictions[\"value\"],\n",
    "        color_style or _COLOR_STYLE,\n",
    "        ax_overall,\n",
    "    )\n",
    "    for idx, name in enumerate(blocks):\n",
    "        axs = axes[i, j]\n",
    "        if \"cleaning\" in name or \"change_point\" in name:\n",
    "            _plot_cleaning_block(\n",
    "                name.split(\":\")[1],\n",
    "                timestamps[:backcast_len],\n",
    "                decomposition[name][\"input remainder\"],\n",
    "                decomposition[name][\"cleaned_series\"],\n",
    "                color_style or _COLOR_STYLE,\n",
    "                idx % 2 == 1 or idx == len(blocks) - 1,\n",
    "                axs,\n",
    "            )\n",
    "        else:\n",
    "            _plot_intermediate_block(\n",
    "                name.split(\":\")[1],\n",
    "                backcast_len,\n",
    "                forecast_len,\n",
    "                timestamps,\n",
    "                decomposition[name][\"input remainder\"],\n",
    "                decomposition[name][\"fitted\"],\n",
    "                decomposition[name][\"forecast\"],\n",
    "                color_style or _COLOR_STYLE,\n",
    "                idx % 2 == 1 or idx == len(blocks) - 1,\n",
    "                axs,\n",
    "            )\n",
    "        if j < 1:\n",
    "            j += 1\n",
    "        else:\n",
    "            i += 1\n",
    "            j = 0\n",
    "    if len(blocks) % 2 != 0:\n",
    "        # delete empty axes\n",
    "        fig.delaxes(axes.flatten()[row * 2 - 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bff7fed4015"
   },
   "source": [
    "## Query test set to get an input time series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2e043dac884a"
   },
   "outputs": [],
   "source": [
    "client = bigquery.Client()\n",
    "table_id = None\n",
    "for task in job.task_details:\n",
    "    if task.task_name == \"set-test-set\":\n",
    "        for k, v in task.outputs.items():\n",
    "            if k == \"artifact\":\n",
    "                table_id = v.artifacts[0].uri[5:]\n",
    "                print(f\"test set table id: {table_id}\")\n",
    "if table_id is None:\n",
    "    raise ValueError(\n",
    "        \"Failed to find test set BQ TABLE. Try looking up the name in the pipeline DAG in the Vertex Console.\"\n",
    "    )\n",
    "\n",
    "query = f\"SELECT * FROM `{table_id}` LIMIT 10\"\n",
    "for row in client.query(query).result():\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5e45301af54c"
   },
   "source": [
    "## Call Endpoint to Generate Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4a399c33e695"
   },
   "outputs": [],
   "source": [
    "instance = {\n",
    "    \"x\": row.x,\n",
    "    \"timestamps\": row.timestamps,\n",
    "    \"city\": row.city,\n",
    "    \"county\": row.county,\n",
    "}\n",
    "out = endpoint.predict(instances=[instance])\n",
    "out.predictions[0][\"value\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ff2489dddd6"
   },
   "source": [
    "## Call Endpoint to Generate Decompositions and Visualize Decomposition Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3b4c2ed62df5"
   },
   "outputs": [],
   "source": [
    "out = endpoint.predict(instances=[instance], parameters={\"include_decomposition\": True})\n",
    "plot_forecasts_from_endpoint(instance, out.predictions[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25fb14f79148"
   },
   "source": [
    "## Clean Up Resources\n",
    "\n",
    "You can delete all the resources you created with the following.\n",
    "\n",
    "Be careful not to accidentally delete resources you might use outside of this demo.\n",
    "\n",
    "If you did not create a brand new bucket and BQ dataset, then do not set `DELETE_BQ_DATASET` and `DELETE_BUCKET` to `True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62da46d90b85"
   },
   "outputs": [],
   "source": [
    "DELETE_VERTEX_RESOURCES = True\n",
    "DELETE_BQ_DATASET = True\n",
    "DELETE_BUCKET = True\n",
    "if DELETE_VERTEX_RESOURCES:\n",
    "    # Undeploy model from endpoint\n",
    "    endpoint.undeploy_all()\n",
    "\n",
    "    # Delete model\n",
    "    model.delete()\n",
    "\n",
    "    # Delete endpoint\n",
    "    endpoint.delete()\n",
    "\n",
    "    # Delete the job\n",
    "    job.delete()\n",
    "\n",
    "    # Delete the tensorboard instance\n",
    "    tensorboard.delete()\n",
    "\n",
    "if DELETE_BQ_DATASET:\n",
    "    # Delete the BQ dataset\n",
    "    client.delete_dataset(dataset, delete_contents=True)\n",
    "\n",
    "if DELETE_BUCKET:\n",
    "    # Delete the bucket\n",
    "    ! gsutil -m rm -r $BUCKET_URI"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "starry_net_pipeline.ipynb",
   "toc_visible": true
  },
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m126",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m126"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel) (Local)",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
